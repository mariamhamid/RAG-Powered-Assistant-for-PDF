{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHK0UxSjISEg",
        "outputId": "349ebf5a-7533-46ca-b1bf-6b65a853204b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/24.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/24.1 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.5/24.1 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/24.1 MB\u001b[0m \u001b[31m265.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m283.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m283.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-community langchain-text-splitters\n",
        "!pip install -q langchain-anthropic langchain-core\n",
        "!pip install -q pymupdf faiss-cpu sentence-transformers\n",
        "!pip install -q huggingface-hub transformers torch python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from urllib.request import urlretrieve\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Ccxy2Ai7l5cG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_FILES = [\n",
        "    \"/content/MachineLearning-supervised.pdf\",\n",
        "    \"/content/MLBasics-Unsupervised.pdf\"\n",
        "]\n",
        "CHUNK_SIZE = 500\n",
        "CHUNK_OVERLAP = 50"
      ],
      "metadata": {
        "id": "q8ot-CAIqAKy"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = load_and_split_multiple_pdfs(PDF_FILES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS6y7_4DtpVM",
        "outputId": "14a941a4-9d61-487a-e1e9-b4fe200bab04"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ“š Loading Multiple PDFs\n",
            "============================================================\n",
            "âœ… Loaded 84 pages\n",
            "âœ… Split into 87 chunks\n",
            "ğŸ§© Sample chunk:\n",
            " Lara Wehbe - TheAIEngineer - July 2024\n",
            "Machine Learning Basics\n",
            "Your AI Starter Kit\n",
            "âœ… Loaded 109 pages\n",
            "âœ… Split into 114 chunks\n",
            "ğŸ§© Sample chunk:\n",
            " Lara Wehbe - TheAIEngineers - August 2024\n",
            "Unsupervised Learning\n",
            "Machine Learning Basics - Continue\n",
            "\n",
            "ğŸ“Š SUMMARY\n",
            "============================================================\n",
            "Total PDFs processed: 2\n",
            "Total chunks created: 201\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_split_single_pdf(pdf_path):\n",
        "    \"\"\"Load and split a single PDF\"\"\"\n",
        "    loader = PyMuPDFLoader(pdf_path)\n",
        "    docs = loader.load()\n",
        "    print(f\"âœ… Loaded {len(docs)} pages\")\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=50\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "    print(f\"âœ… Split into {len(chunks)} chunks\")\n",
        "    if chunks:\n",
        "        print(\"ğŸ§© Sample chunk:\\n\", chunks[0].page_content[:300])\n",
        "    else:\n",
        "        print(\"âš ï¸ No text extracted â€” check if the PDF contains selectable text.\")\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Run\n",
        "if __name__ == \"__main__\":\n",
        "    chunks = load_and_split_multiple_pdfs(PDF_FILES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEHZBuM_zl8v",
        "outputId": "d5634bad-9a53-47c7-996e-cc086c1e6bfa"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ğŸ“š Loading Multiple PDFs\n",
            "============================================================\n",
            "âœ… Loaded 84 pages\n",
            "âœ… Split into 87 chunks\n",
            "ğŸ§© Sample chunk:\n",
            " Lara Wehbe - TheAIEngineer - July 2024\n",
            "Machine Learning Basics\n",
            "Your AI Starter Kit\n",
            "âœ… Loaded 109 pages\n",
            "âœ… Split into 114 chunks\n",
            "ğŸ§© Sample chunk:\n",
            " Lara Wehbe - TheAIEngineers - August 2024\n",
            "Unsupervised Learning\n",
            "Machine Learning Basics - Continue\n",
            "\n",
            "ğŸ“Š SUMMARY\n",
            "============================================================\n",
            "Total PDFs processed: 2\n",
            "Total chunks created: 201\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load the PDF\n",
        "loader = PyMuPDFLoader(PDF_FILES[0])\n",
        "docs_before_split = loader.load()\n",
        "print(f\"Loaded {len(docs_before_split)} documents\")\n",
        "\n",
        "# Split documents\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "docs_after_split = text_splitter.split_documents(docs_before_split)\n",
        "print(f\"Split into {len(docs_after_split)} chunks\")\n",
        "print(\"Sample chunk:\\n\", docs_after_split[0].page_content[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT1djkDp0WaQ",
        "outputId": "c5a2d4ad-a4b3-453d-ccf0-1cc1c4ba7e7c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 84 documents\n",
            "Split into 95 chunks\n",
            "Sample chunk:\n",
            " Lara Wehbe - TheAIEngineer - July 2024\n",
            "Machine Learning Basics\n",
            "Your AI Starter Kit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFDirectoryLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "\n",
        "def load_and_split_documents(path):\n",
        "    \"\"\"\n",
        "    Load PDFs (single file or folder) and split them into chunks\n",
        "    \"\"\"\n",
        "    # Detect if it's a file or directory\n",
        "    if os.path.isfile(path) and path.endswith(\".pdf\"):\n",
        "        loader = PyMuPDFLoader(path)\n",
        "    elif os.path.isdir(path):\n",
        "        loader = PyPDFDirectoryLoader(path)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid path: provide a PDF file or folder of PDFs.\")\n",
        "\n",
        "    docs_before_split = loader.load()\n",
        "    print(f\"Loaded {len(docs_before_split)} documents\")\n",
        "\n",
        "    # Split documents\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=300,\n",
        "        chunk_overlap=50\n",
        "    )\n",
        "    docs_after_split = text_splitter.split_documents(docs_before_split)\n",
        "\n",
        "    print(f\"Split into {len(docs_after_split)} chunks\")\n",
        "    if docs_after_split:\n",
        "        print(\"Sample chunk:\\n\", docs_after_split[0].page_content[:300])\n",
        "    else:\n",
        "        print(\"No text found. Check if PDFs contain readable text.\")\n",
        "\n",
        "    return docs_after_split\n",
        "\n",
        "# Run example\n",
        "if __name__ == \"__main__\":\n",
        "    chunks = load_and_split_documents(PDF_FILES[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcj5-tYL0-TF",
        "outputId": "e6d010ea-e994-4fea-aa5a-443d3e0388a9"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 84 documents\n",
            "Split into 95 chunks\n",
            "Sample chunk:\n",
            " Lara Wehbe - TheAIEngineer - July 2024\n",
            "Machine Learning Basics\n",
            "Your AI Starter Kit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(docs):\n",
        "    \"\"\"Create FAISS vector store with embeddings\"\"\"\n",
        "    # Initialize embeddings\n",
        "    embeddings = HuggingFaceBgeEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={'device': 'cpu'},\n",
        "        encode_kwargs={'normalize_embeddings': True}\n",
        "    )\n",
        "\n",
        "    # Create vector store\n",
        "    vector_db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "    return vector_db\n"
      ],
      "metadata": {
        "id": "5twE76KB1dSD"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def setup_llm():\n",
        "    \"\"\"Setup HuggingFace Pipeline LLM\"\"\"\n",
        "    # Use HuggingFacePipeline instead of HuggingFaceHub\n",
        "    llm = HuggingFacePipeline.from_model_id(\n",
        "        model_id=\"google/flan-t5-base\",\n",
        "        task=\"text2text-generation\",\n",
        "        model_kwargs={\n",
        "            \"temperature\": 0.2,\n",
        "            \"max_length\": 128,\n",
        "            \"do_sample\": True\n",
        "        },\n",
        "        pipeline_kwargs={\n",
        "            \"max_new_tokens\": 128\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return llm"
      ],
      "metadata": {
        "id": "tgHOJsU31huY"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def create_rag_chain(vector_db, llm):\n",
        "    \"\"\"Create RAG chain using modern LCEL (no deprecated RetrievalQA)\"\"\"\n",
        "\n",
        "    # Create retriever\n",
        "    retriever = vector_db.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 3}\n",
        "    )\n",
        "\n",
        "    # Define custom prompt template\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
        "1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer but you may want to check the following links\".\n",
        "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:{question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Format retrieved documents\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Create chain using LCEL (modern approach)\n",
        "    rag_chain = (\n",
        "        {\n",
        "            \"context\": retriever | format_docs,\n",
        "            \"question\": RunnablePassthrough()\n",
        "        }\n",
        "        | PROMPT\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return rag_chain, retriever"
      ],
      "metadata": {
        "id": "VFDY7G1g1ljj"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qQYMXmD1nRE",
        "outputId": "4c9b8e6b-e11a-4b0b-8289-131e683c347f"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸš€ RAG SYSTEM STARTING\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Load documents\n",
        "    chunks = load_and_split_multiple_pdfs(PDF_FILES)\n",
        "\n",
        "    if not chunks:\n",
        "        print(\"âŒ No documents loaded!\")\n",
        "        return\n",
        "\n",
        "    # Create vector store\n",
        "    vector_db = create_vector_store(chunks)\n",
        "\n",
        "    # Setup LLM\n",
        "    llm = setup_llm()\n",
        "\n",
        "    # Create chain\n",
        "    chain, retriever = create_rag_chain(vector_db, llm)\n",
        "\n",
        "    # Test query\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ§ª TEST QUERY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    test_query = \"What is supervised learning?\"\n",
        "    print(f\"Query: {test_query}\\n\")\n",
        "\n",
        "    print(\"Searching...\")\n",
        "    answer = chain.invoke(test_query)\n",
        "    print(f\"Answer: {answer}\\n\")\n",
        "\n",
        "    # Interactive mode\n",
        "    print(\"=\"*60)\n",
        "    print(\"ğŸ’¬ INTERACTIVE MODE\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Type 'quit' to exit\\n\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"Your question: \").strip()\n",
        "\n",
        "        if question.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"ğŸ‘‹ Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not question:\n",
        "            continue\n",
        "\n",
        "        print(\"\\nâ³ Thinking...\")\n",
        "        answer = chain.invoke(question)\n",
        "        print(f\"Answer: {answer}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# RUN\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWYixtxf1pcq",
        "outputId": "e52931a3-d9b6-488e-d4fc-e5fe844ba91e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸš€ RAG SYSTEM STARTING\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "ğŸ“š Loading Multiple PDFs\n",
            "============================================================\n",
            "âœ… Loaded 84 pages\n",
            "âœ… Split into 87 chunks\n",
            "ğŸ§© Sample chunk:\n",
            " Lara Wehbe - TheAIEngineer - July 2024\n",
            "Machine Learning Basics\n",
            "Your AI Starter Kit\n",
            "âœ… Loaded 109 pages\n",
            "âœ… Split into 114 chunks\n",
            "ğŸ§© Sample chunk:\n",
            " Lara Wehbe - TheAIEngineers - August 2024\n",
            "Unsupervised Learning\n",
            "Machine Learning Basics - Continue\n",
            "\n",
            "ğŸ“Š SUMMARY\n",
            "============================================================\n",
            "Total PDFs processed: 2\n",
            "Total chunks created: 201\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ§ª TEST QUERY\n",
            "============================================================\n",
            "Query: What is supervised learning?\n",
            "\n",
            "Searching...\n",
            "Answer: a category of Machine Learning that uses labeled datasets to train algorithms to predict outcomes and recognize patterns.\n",
            "\n",
            "============================================================\n",
            "ğŸ’¬ INTERACTIVE MODE\n",
            "============================================================\n",
            "Type 'quit' to exit\n",
            "\n",
            "Your question: What is supervised learning\n",
            "\n",
            "â³ Thinking...\n",
            "Answer: a category of Machine Learning that uses labeled datasets to train algorithms to predict outcomes and recognize patterns.\n",
            "\n",
            "Your question: what is machine learning\n",
            "\n",
            "â³ Thinking...\n",
            "Answer: ML is a subset of AI that focuses on the development of algorithms that allow computers to learn from and make predictions based on data.\n",
            "\n",
            "Your question: quit\n",
            "ğŸ‘‹ Goodbye!\n"
          ]
        }
      ]
    }
  ]
}